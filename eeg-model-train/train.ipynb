{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bfa566-1433-432d-8aa6-b0430bea0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647 1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysurkov/.conda/envs/ml_cb/lib/python3.10/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Код для реализации обучения модели нейросети Artifact Removal Transformer для очистки ЭЭГ от артфектов\n",
    "'''\n",
    "import re \n",
    "import gc \n",
    "import os\n",
    "import sys\n",
    "import mne \n",
    "import glob\n",
    "import math\n",
    "import types\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import EEGArtifactDataset\n",
    "from metrics import compute_snr, frequency_band_error_db, evaluate, print_metrics\n",
    "from tf_model import (EncoderDecoder, Encoder, Decoder, Generator, EncoderLayer, DecoderLayer, MultiHeadedAttention, \n",
    "            PositionwiseFeedForward, ExpandConv, PositionalEncoding, make_model)\n",
    "\n",
    "\n",
    "\n",
    "sol_files     = glob.glob(\"ICA/**/ica_solution.fif\") # Объект MNE хранящий решение ICA разложения (матрица W)\n",
    "iclabel_files = glob.glob(\"ICA/**/ICLabel_info.pkl\") # файл хранящий информацию о raw файле поданного на ICA, ICA sources, \n",
    "                                                     # и классификацию компонент\n",
    "print(len(sol_files), len(iclabel_files))\n",
    "assert len(sol_files) == len(iclabel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f23bc-6056-4028-a2bf-13120ff6f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_files = {}\n",
    "for sol, icl in zip(sol_files, iclabel_files):\n",
    "    # Извлекаем sub-XXX из пути\n",
    "    subject = Path(sol).parent.name.split('_')[0]  # Например, 'sub-001'\n",
    "    if subject not in subject_files:\n",
    "        subject_files[subject] = {'sol': [], 'icl': []}\n",
    "    subject_files[subject]['sol'].append(sol)\n",
    "    subject_files[subject]['icl'].append(icl)\n",
    "\n",
    "subjects = list(subject_files.keys())\n",
    "print(f\"Total subjects: {len(subjects)}\")\n",
    "print(f\"First participants: {subjects[:5]}\")\n",
    "\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "random_seed = 42\n",
    "\n",
    "train_subjects, temp_subjects = train_test_split(\n",
    "    subjects, train_size=train_ratio, random_state=random_seed\n",
    ")\n",
    "valid_subjects, test_subjects = train_test_split(\n",
    "    temp_subjects, train_size=valid_ratio/(valid_ratio + test_ratio), random_state=random_seed\n",
    ")\n",
    "\n",
    "train_sol_files = []\n",
    "train_iclabel_files = []\n",
    "valid_sol_files = []\n",
    "valid_iclabel_files = []\n",
    "test_sol_files = []\n",
    "test_iclabel_files = []\n",
    "\n",
    "for subject in train_subjects:\n",
    "    train_sol_files.extend(subject_files[subject]['sol'])\n",
    "    train_iclabel_files.extend(subject_files[subject]['icl'])\n",
    "for subject in valid_subjects:\n",
    "    valid_sol_files.extend(subject_files[subject]['sol'])\n",
    "    valid_iclabel_files.extend(subject_files[subject]['icl'])\n",
    "for subject in test_subjects:\n",
    "    test_sol_files.extend(subject_files[subject]['sol'])\n",
    "    test_iclabel_files.extend(subject_files[subject]['icl'])\n",
    "\n",
    "print(f\"Train: {len(train_subjects)} subjects, {len(train_sol_files)} files\")\n",
    "print(f\"Valid: {len(valid_subjects)} subjects, {len(valid_sol_files)} files\")\n",
    "print(f\"Test: {len(test_subjects)} subjects, {len(test_sol_files)} files\")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "\n",
    "def pad_collate(batch):\n",
    "    T     = batch[0][0].size(1)\n",
    "    clean_list = []\n",
    "    noisy_list = []\n",
    "    for clean, noisy in batch:\n",
    "        C_i = clean.size(0)\n",
    "        pad = C_max - C_i\n",
    "        if pad > 0:\n",
    "            # pad = (pad_T_left, pad_T_right, pad_C_left, pad_C_right)\n",
    "            clean  = F.pad(clean,  (0,0, 0,pad))\n",
    "            noisy  = F.pad(noisy,  (0,0, 0,pad))\n",
    "        clean_list.append(clean)\n",
    "        noisy_list.append(noisy)\n",
    "    \n",
    "    clean_batch = torch.stack(clean_list, dim=0)  # (B, C_max, T)\n",
    "    noisy_batch = torch.stack(noisy_list, dim=0)  # (B, C_max, T)\n",
    "    return clean_batch, noisy_batch\n",
    "\n",
    "def check_dataset(dataset, ch=0, idx=32):\n",
    "    clean, noisy = dataset[idx]\n",
    "    plt.figure()\n",
    "    plt.plot(noisy[ch].cpu(), label='Noisy signal', color='orange')\n",
    "    plt.plot(clean[ch].cpu(), label='Clean signal', color='b')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude (normalized)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "n_jobs = 25 #os.cpu_count()\n",
    "segment_length = 1024\n",
    "brain_threshold = 0.8\n",
    "device_dataset = 'cpu'\n",
    "target_sfreq=256\n",
    "min_brain_components=6\n",
    "max_other_componnets=80\n",
    "print(f\"Available CPU cores: {n_jobs}\")\n",
    "\n",
    "\n",
    "train_dataset = EEGArtifactDataset(train_sol_files, train_iclabel_files, segment_length=segment_length, brain_threshold=brain_threshold, \n",
    "                             n_jobs=n_jobs, device=device_dataset, target_sfreq=target_sfreq, \n",
    "                             min_brain_components=min_brain_components, max_other_componnets=max_other_componnets)\n",
    "valid_dataset = EEGArtifactDataset(valid_sol_files, valid_iclabel_files, segment_length=segment_length, brain_threshold=brain_threshold, \n",
    "                             n_jobs=n_jobs, device=device_dataset, target_sfreq=target_sfreq, \n",
    "                             min_brain_components=min_brain_components, max_other_componnets=max_other_componnets)\n",
    "test_dataset = EEGArtifactDataset(test_sol_files, test_iclabel_files, segment_length=segment_length, brain_threshold=brain_threshold, \n",
    "                             n_jobs=n_jobs, device=device_dataset, target_sfreq=target_sfreq, \n",
    "                             min_brain_components=min_brain_components, max_other_componnets=max_other_componnets)\n",
    "\n",
    "C_max = max(*[ica.get_components().shape[0] for ica in train_dataset.icas], \n",
    "            *[ica.get_components().shape[0] for ica in valid_dataset.icas],\n",
    "            *[ica.get_components().shape[0] for ica in test_dataset.icas])\n",
    "\n",
    "print(f\"Size of Train Dataset: {len(train_dataset)}\")\n",
    "print(f\"Size of Valid Dataset: {len(valid_dataset)}\")\n",
    "print(f\"Size of Test Dataset: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=pad_collate,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=pad_collate,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=pad_collate,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"================TRAIN===================\")\n",
    "check_dataset(train_dataset, ch=0, idx=0)\n",
    "print(f\"================VALID===================\")\n",
    "check_dataset(valid_dataset, ch=0, idx=0)\n",
    "print(f\"================TEST===================\")\n",
    "check_dataset(test_dataset, ch=0, idx=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba55b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysurkov/Vasya/tf_model.py:340: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(p)\n",
      "/tmp/job-2691887/ipykernel_229288/289154878.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"checkpoint.pth.tar\", map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = 30\n",
    "tgt_vocab = 30\n",
    "N = 2\n",
    "d_model = 128\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = make_model(src_vocab=src_vocab, tgt_vocab=tgt_vocab, N=N, d_model=d_model)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoint.pth.tar\", map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb01da5-a03e-4614-b161-89d609ed4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed in (model.src_embed, model.tgt_embed):\n",
    "    exp = embed[0]            # это ваш ExpandConv\n",
    "    exp.d_model = d_model     # чтобы было в локальном scope\n",
    "    exp._n_chan = C_max\n",
    "    exp.lut = nn.Conv1d(in_channels=C_max,\n",
    "                        out_channels=d_model,\n",
    "                        kernel_size=1,\n",
    "                        bias=True)\n",
    "\n",
    "    def expand_forward(self, x):\n",
    "        # x: (B, C_i, T)\n",
    "        C_i = x.size(1)\n",
    "        # берём только первые C_i каналов из весов [d_model × C_max × 1]\n",
    "        w = self.lut.weight[:, :C_i, :]\n",
    "        b = self.lut.bias\n",
    "        y = F.conv1d(x, w, bias=b, stride=self.lut.stride, padding=self.lut.padding)\n",
    "        # → (B, d_model, T) → (B, T, d_model) и масштаб\n",
    "        return y.permute(0, 2, 1) * math.sqrt(self.d_model)\n",
    "\n",
    "    exp.forward = types.MethodType(expand_forward, exp)\n",
    "\n",
    "model.generator._n_chan = C_max    # <–– и здесь\n",
    "model.generator.proj = nn.Linear(in_features=d_model,\n",
    "                                 out_features=C_max,\n",
    "                                 bias=True)\n",
    "\n",
    "def generator_forward(self, x):\n",
    "    C_i = self._n_chan\n",
    "    W = self.proj.weight[:C_i, :]\n",
    "    b = self.proj.bias[:C_i]\n",
    "    B, T, D = x.shape\n",
    "    flat = x.reshape(-1, D)          # (B*T, d_model)\n",
    "    out = flat @ W.t() + b           # (B*T, C_i)\n",
    "    return out.view(B, T, C_i)\n",
    "\n",
    "model.generator.forward = types.MethodType(generator_forward, model.generator)\n",
    "\n",
    "def init_xavier(m):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Linear)) and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.src_embed[0].apply(init_xavier)\n",
    "model.tgt_embed[0].apply(init_xavier)\n",
    "model.generator.apply(init_xavier)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for m in (model.src_embed[0].lut,\n",
    "          model.tgt_embed[0].lut,\n",
    "          model.generator.proj):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51b174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.4\n",
      "Device: cuda\n",
      "CUDA mem: (33755365376, 34079899648)\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"CUDA mem:\", torch.cuda.mem_get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03532db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/259 [03:24<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cf62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 778/778 [07:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 - Train Metrics:\n",
      "+------------------+-----------+\n",
      "| Metric           |     Value |\n",
      "+==================+===========+\n",
      "| Loss             |  0.029763 |\n",
      "+------------------+-----------+\n",
      "| SNR (dB)         | 11.87     |\n",
      "+------------------+-----------+\n",
      "| L1 Time          |  0.121839 |\n",
      "+------------------+-----------+\n",
      "| L2 Time          |  0.029763 |\n",
      "+------------------+-----------+\n",
      "| L1 Freq          |  2.87426  |\n",
      "+------------------+-----------+\n",
      "| L2 Freq          | 18.1603   |\n",
      "+------------------+-----------+\n",
      "| delta Error (dB) |  0.97     |\n",
      "+------------------+-----------+\n",
      "| theta Error (dB) |  0.98     |\n",
      "+------------------+-----------+\n",
      "| alpha Error (dB) |  0.98     |\n",
      "+------------------+-----------+\n",
      "| beta Error (dB)  |  0.99     |\n",
      "+------------------+-----------+\n",
      "| gamma Error (dB) |  1        |\n",
      "+------------------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [01:21<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 - Valid Metrics:\n",
      "+------------------+-----------+\n",
      "| Metric           |     Value |\n",
      "+==================+===========+\n",
      "| Loss             |  0.01314  |\n",
      "+------------------+-----------+\n",
      "| SNR (dB)         | 15.26     |\n",
      "+------------------+-----------+\n",
      "| L1 Time          |  0.077263 |\n",
      "+------------------+-----------+\n",
      "| L2 Time          |  0.01314  |\n",
      "+------------------+-----------+\n",
      "| L1 Freq          |  1.39747  |\n",
      "+------------------+-----------+\n",
      "| L2 Freq          | 10.3552   |\n",
      "+------------------+-----------+\n",
      "| delta Error (dB) |  0.97     |\n",
      "+------------------+-----------+\n",
      "| theta Error (dB) |  0.98     |\n",
      "+------------------+-----------+\n",
      "| alpha Error (dB) |  0.98     |\n",
      "+------------------+-----------+\n",
      "| beta Error (dB)  |  0.97     |\n",
      "+------------------+-----------+\n",
      "| gamma Error (dB) |  0.95     |\n",
      "+------------------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 632/778 [05:35<01:11,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import metrics\n",
    "importlib.reload(metrics)\n",
    "\n",
    "num_epochs = 10\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    phases = {\n",
    "        \"train\": train_dataloader,\n",
    "        \"valid\": valid_dataloader\n",
    "    }\n",
    "    \n",
    "    for mode, dataloader in phases.items():\n",
    "        if mode == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        snr_total = 0\n",
    "        l1_total = 0\n",
    "        l2_total = 0\n",
    "        l1_freq_total = 0\n",
    "        l2_freq_total = 0\n",
    "        band_errors_total = {band: 0 for band in [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]}\n",
    "        \n",
    "        for (target, noised) in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "            noised = noised.to(device)\n",
    "            target = target.to(device)\n",
    "            src_mask = torch.ones((noised.size(0), 1, noised.size(2)), device=device)  # [batch, 1, 2000]\n",
    "            tgt_mask = None\n",
    "\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                output = model(noised, target, src_mask, tgt_mask).permute(0, 2, 1)\n",
    "                #print(f\"Output shape: {output.shape}, Target shape: {target.shape}\")\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    output = model(noised, target, src_mask, tgt_mask).permute(0, 2, 1)\n",
    "                    loss = criterion(output, target)\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            epoch_loss += current_loss\n",
    "            \n",
    "            # Evaluate metrics\n",
    "            with torch.no_grad():\n",
    "                snr, band_errors, l1_time, l2_time, l1_freq, l2_freq = evaluate(target, output, target_sfreq)\n",
    "                snr_total += snr\n",
    "                l1_total += l1_time\n",
    "                l2_total += l2_time\n",
    "                l1_freq_total += l1_freq\n",
    "                l2_freq_total += l2_freq\n",
    "                for band_name in band_errors_total:\n",
    "                    band_errors_total[band_name] += band_errors[band_name]\n",
    "            \n",
    "        num_batches = len(dataloader)\n",
    "        # Compute averages\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_snr = snr_total / num_batches\n",
    "        avg_l1 = l1_total / num_batches\n",
    "        avg_l2 = l2_total / num_batches\n",
    "        avg_l1_freq = l1_freq_total / num_batches\n",
    "        avg_l2_freq = l2_freq_total / num_batches\n",
    "        avg_band_errors = {band: total / num_batches for band, total in band_errors_total.items()}\n",
    "        \n",
    "        # Print formatted metrics\n",
    "        print_metrics(epoch, mode, avg_loss, avg_snr, avg_band_errors, avg_l1, avg_l2, avg_l1_freq, avg_l2_freq)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df31f1-9e22-4fc9-b404-f404bd5a4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_signals(input_tensor, pred_tensor, target_tensor, sfreq=256, n_signals= 1):\n",
    "    inputs  = input_tensor[:n_signals].cpu().numpy()\n",
    "    preds   = pred_tensor[:n_signals].cpu().detach().numpy()\n",
    "    targets = target_tensor[:n_signals].cpu().numpy()\n",
    "    times = np.arange(inputs.shape[1]) / sfreq  # шкала времени в секундах\n",
    "\n",
    "    titles = [\"Noised Input\", \"Model Prediction\", \"Ground Truth\"]\n",
    "    datas = [inputs, preds, targets]\n",
    "    for i in range(n_signals):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for idx, data, title in zip(range(1, 4), datas, titles):\n",
    "            plt.subplot(3, 1, idx)\n",
    "            plt.plot(times, data[i])\n",
    "            plt.title(f\"Signal {i+1} — \" + title)\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "data_idx = 21\n",
    "chan = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (target, noised) in enumerate(test_dataloader):\n",
    "        if idx == data_idx:\n",
    "            noised = noised.to(device)          # [batch, channels, time]\n",
    "            target = target.to(device)\n",
    "            src_mask = torch.ones((noised.size(0), 1, noised.size(2)), device=device)\n",
    "            # Prediction\n",
    "            output = model(noised, target, src_mask, None).permute(0, 2, 1)  # [batch, time, channels]\n",
    "            # Для простоты переводим в [batch, time]\n",
    "            input_sig  = noised[:, chan, :]\n",
    "            pred_sig   = output[:, chan, :]   \n",
    "            target_sig = target[:, chan, :]     \n",
    "            \n",
    "            plot_signals(input_sig, pred_sig, target_sig, sfreq=256, n_signals=2)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecf291-1711-4366-939b-6a5e40abb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = {\n",
    "    'epoch': epoch,                              # текущая эпоха\n",
    "    'model_state_dict': model.state_dict(),      # веса модели\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # состояние оптимизатора\n",
    "    'loss': loss.item(),                          # текущая потеря (для истории)\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'my_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f79245-288d-4db6-bc13-f96809131894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
